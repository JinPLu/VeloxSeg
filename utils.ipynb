{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Configuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = {\n",
    "    \"patch_size\": {\n",
    "        \"AutoPETII\": [96, 96, 96],\n",
    "        \"Hecktor2022\": [128, 128, 64],\n",
    "        \"BraTS2021\": [96, 96, 96],\n",
    "        \"MSD2019\": [96, 96, 96],\n",
    "    },\n",
    "    \"spacing\": {\n",
    "        \"AutoPETII\": [2.03642, 2.03642, 3],\n",
    "        \"Hecktor2022\": [1, 1, 1],\n",
    "        \"BraTS2021\": [1, 1, 1],\n",
    "        \"AutoPETIII\": [1, 1, 1],\n",
    "        \"MSD2019\": [1, 1, 1]\n",
    "    },\n",
    "    \n",
    "    \"batch_size\": 2, \n",
    "    # You can adjust the batch size based on the GPU memory usage of your model,\n",
    "    # please change lr, if you change batch_size: lr = (5e-4) * sqrt(batch_size / 8)\n",
    "    # Note: Each case samples 2 patches, so the input batch_size is 4 * 2\n",
    "    \n",
    "    # loss weights\n",
    "    \"deep_Loss_weight\": [1, 1, 1, 1],\n",
    "    \"RC_Loss_weight\": 0.5,\n",
    "    \"Feature_Loss_weight\": 2.0,\n",
    "    \n",
    "    # train, val, test rate\n",
    "    \"train_rate\": 0.6,\n",
    "    \"val_rate\": 0.2,\n",
    "    \"epochs\": 300,\n",
    "    \n",
    "    # show deep metric\n",
    "    \"show_deep_metric\": True,\n",
    "    \n",
    "    # save model interval\n",
    "    \"save_model_interval\": 5,\n",
    "    \"val_interval\": 5,\n",
    "    \n",
    "    # optimizer and scheduler\n",
    "    \"optimizer\": {\n",
    "        \"optimizer_type\": \"adamw\",\n",
    "        \"optimizer_args\": {\n",
    "            \"lr\": 2.5e-4,\n",
    "            \"weight_decay\": 0.01\n",
    "        }\n",
    "    },\n",
    "    \"warmup_scheduler\": {\n",
    "        \"enabled\": True,\n",
    "        \"warmup_epochs\": 10\n",
    "    },\n",
    "    \"train_scheduler\": {\n",
    "        \"scheduler_type\": \"cosine_annealing\",\n",
    "        \"scheduler_args\": {\n",
    "            \"epochs\": 290, # epoch - warmup_epoch\n",
    "            \"min_lr\": 6e-06\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # save path\n",
    "    \"save_path\": \"./save/\",\n",
    "    \"log_path\": \"./logs/\",\n",
    "    \"config_path\": \"./config/\",\n",
    "\n",
    "    # dataset path\n",
    "    \"dataset_path\": {\n",
    "        \"AutoPETII\": {\n",
    "            \"ct_path\": \"./datasets/AutoPETII_spac_norm/imagesTr/*0001.nii.gz\",\n",
    "            \"pet_path\": \"./datasets/AutoPETII_spac_norm/imagesTr/*0000.nii.gz\",\n",
    "            \"label_path\": \"./datasets/AutoPETII_spac_norm/labelsTr/*.nii.gz\"\n",
    "        },\n",
    "        \"Hecktor2022\": {\n",
    "            \"ct_path\": \"./datasets/Hecktor2022_spac_norm/imagesTr/*CT.nii.gz\",\n",
    "            \"pet_path\": \"./datasets/Hecktor2022_spac_norm/imagesTr/*PT.nii.gz\",\n",
    "            \"label_path\": \"./datasets/Hecktor2022_spac_norm/labelsTr/*.nii.gz\"\n",
    "        },\n",
    "        \"BraTS2021\":{\n",
    "            \"flair_path\":\"./datasets/BraTS2021_spac_norm/*/*_flair.nii.gz\",\n",
    "            \"t1_path\":\"./datasets/BraTS2021_spac_norm/*/*_t1.nii.gz\",\n",
    "            \"t1ce_path\":\"./datasets/BraTS2021_spac_norm/*/*_t1ce.nii.gz\",\n",
    "            \"t2_path\":\"./datasets/BraTS2021_spac_norm/*/*_t2.nii.gz\",\n",
    "            \"label_path\":\"./datasets/BraTS2021_spac_norm/*/*_seg.nii.gz\",\n",
    "        },\n",
    "        \"MSD2019\": {\n",
    "            \"data_path\": \"./datasets/MSD2019_Task01/imagesTr/*.nii.gz\",\n",
    "            \"label_path\": \"./datasets/MSD2019_Task01/labelsTr/*.nii.gz\"\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "with open('./config/train_config_bs4.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = {\n",
    "    \"result_metric_path\": \"./result/metric\",\n",
    "    \"result_pred_path\": \"./result/prediction\"\n",
    "}\n",
    "\n",
    "with open('./config/test_config.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoPETII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 基准模型配置\n",
    "data = {\n",
    "    \"UNet\":{\n",
    "        \"in_channels\": 2,\n",
    "        \"out_channels\": 2,\n",
    "    },\n",
    "    \"VNet\":{\n",
    "        \"in_channels\": 2,\n",
    "        \"out_channels\": 2,\n",
    "    },\n",
    "    \"MedNeXt\":{\n",
    "        \"num_input_channels\": 2, \n",
    "        \"num_classes\": 2, \n",
    "    },\n",
    "    \"UNETR\":{\n",
    "        \"in_channels\": 2,\n",
    "        \"out_channels\": 2,\n",
    "        \"img_size\": (96, 96, 96)\n",
    "    },\n",
    "    \"SwinUNETR\":{\n",
    "        \"in_channels\": 2,\n",
    "        \"out_channels\": 2\n",
    "    },\n",
    "    \"NestedFormer\":{\n",
    "        \"model_num\": 2,\n",
    "        \"out_channels\": 2,\n",
    "        \"image_size\": (96, 96, 96)\n",
    "    },\n",
    "    \"A2FSeg\":{\n",
    "        \"modality_num\": 2,\n",
    "        \"base_num_features\": 16,\n",
    "        \"num_classes\": 2,\n",
    "        \"num_pool\": 5,\n",
    "        \"num_conv_per_stage\": 2,\n",
    "        \"feat_map_mul_on_downscale\": 2,\n",
    "        \"norm_op_kwargs\": {'eps': 1e-05, 'affine': True},\n",
    "        \"dropout_op_kwargs\": {'p': 0, 'inplace': True},\n",
    "        \"nonlin_kwargs\": {'negative_slope': 0.01, 'inplace': True},\n",
    "        \"deep_supervision\": True,\n",
    "        \"dropout_in_localization\": False,\n",
    "        \"pool_op_kernel_sizes\": [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]],\n",
    "        \"conv_kernel_sizes\": [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]],\n",
    "        \"upscale_logits\": False,\n",
    "        \"convolutional_pooling\": True,\n",
    "        \"convolutional_upsampling\": True,\n",
    "    },\n",
    "    \"SlimUNETR\":{\n",
    "        \"in_channels\": 2,\n",
    "        \"out_channels\": 2,\n",
    "        \"embed_dim\": 96,\n",
    "        \"embedding_dim\": 27\n",
    "    },\n",
    "    \"SegFormer\":{\n",
    "        \"in_channels\": 2,\n",
    "        \"num_classes\": 2\n",
    "    },\n",
    "    \"UNETRpp\":{\n",
    "        \"in_channels\": 2, \n",
    "        \"out_channels\": 2,\n",
    "        \"patch_size\" : [96, 96, 96],\n",
    "        \"feature_size\": 16,\n",
    "        \"hidden_size\": 256,\n",
    "        \"num_heads\": 4,\n",
    "        \"depths\": [3, 3, 3, 3],\n",
    "        \"dims\": [32, 64, 128, 256],\n",
    "        \"do_ds\": True\n",
    "    },\n",
    "    \"VSmTrans\":{\n",
    "        \"in_channels\": 2,\n",
    "        \"out_channels\": 2,\n",
    "        \"feature_size\": 24,\n",
    "        \"split_size\": [1, 2, 3, 4],\n",
    "        \"window_size\": 6,\n",
    "        \"num_heads\": [3, 6, 12, 24],\n",
    "        \"img_size\": [96, 96, 96],\n",
    "        \"depths\": [2, 2, 2, 2],\n",
    "        \"patch_size\": (2, 2, 2),\n",
    "        \"do_ds\": True\n",
    "    },\n",
    "    \"HDense\":{\n",
    "        \"in_channels\": 2, \n",
    "        \"n_cls\": 2,\n",
    "        \"image_size\": (96, 96, 96),\n",
    "        \"transformer_depth\": 24\n",
    "    },\n",
    "    \n",
    "    \"SuperLightNet\":{\n",
    "        \"init_channels\": 2,\n",
    "        \"class_nums\": 2,\n",
    "        \"depths_unidirectional\": 'small'\n",
    "    },\n",
    "    \n",
    "    \"HCMA-UNet\":{\n",
    "        \"in_channels\": 2,\n",
    "        \"n_classes\": 2,\n",
    "        \"patch_ini\": [96, 96, 96],\n",
    "        \"predict_mode\": True\n",
    "    },\n",
    "    \n",
    "    \"U-KAN\":{\n",
    "        \"num_classes\": 2, \n",
    "        \"input_channels\": 2, \n",
    "        \"img_size\": [96, 96, 96], \n",
    "        \"embed_dims\": [128, 160, 256],\n",
    "    },\n",
    "    \n",
    "    \"U-RWKV\":{\n",
    "        \"input_channel\": 2, \n",
    "        \"num_classes\": 2\n",
    "    },\n",
    "\n",
    "    \"VeloxSeg\":{\n",
    "        \"input_size\": [96, 96, 96],\n",
    "        \"patch_size\": 4,\n",
    "        \"in_ch\": [1, 1],\n",
    "        \"n_classes\": 2,\n",
    "        \"base_ch\": 16,\n",
    "        \n",
    "        \"conv_depths\": [1, 1, 1, 1],\n",
    "        \"kernel_sizes\": [1, 3, 5],\n",
    "        \"min_dim_group\": [4, 8, 8, 16],\n",
    "        \"conv_expansion_factor\": [3, 3, 2, 2],\n",
    "        \n",
    "        \"attn_base_ch\": 16, \n",
    "        \"depths\": [1, 1, 1, 1],\n",
    "        \"min_big_window_sizes\": [[3, 3, 3], [6, 6, 6], [3, 3, 3], [3, 3, 3]],\n",
    "        \"min_small_window_sizes\": [[1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1]],\n",
    "        \"min_dim_head\": [4, 8, 8, 16],\n",
    "        \"ffn_expansion_ratio\": [3, 3, 2, 2],\n",
    "        \"num_heads\": [1, 2, 2, 4],\n",
    "        \n",
    "        \"proj_drop\":0.1,\n",
    "        \"conv_drop\":0.1,\n",
    "        \"spatial_dim\":  3\n",
    "    },\n",
    "}\n",
    "with open('./config/models_config_autopetii.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hecktor2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = {\n",
    "    \"UNet\":{\n",
    "        \"in_channels\": 2,\n",
    "        \"out_channels\": 2,\n",
    "    },\n",
    "    \"VNet\":{\n",
    "        \"in_channels\": 2,\n",
    "        \"out_channels\": 2,\n",
    "    },\n",
    "    \"MedNeXt\":{\n",
    "        \"num_input_channels\": 2, \n",
    "        \"num_classes\": 2, \n",
    "    },\n",
    "    \"UNETR\":{\n",
    "        \"in_channels\": 2,\n",
    "        \"out_channels\": 2,\n",
    "        \"img_size\": (128, 128, 64)\n",
    "    },\n",
    "    \"SwinUNETR\":{\n",
    "        \"in_channels\": 2,\n",
    "        \"out_channels\": 2\n",
    "    },\n",
    "    \"NestedFormer\":{\n",
    "        \"model_num\": 2,\n",
    "        \"out_channels\": 2,\n",
    "        \"image_size\": (128, 128, 64),\n",
    "        \"window_size\": (4, 4, 2)\n",
    "    },\n",
    "    \"A2FSeg\":{\n",
    "        \"modality_num\": 2,\n",
    "        \"base_num_features\": 16,\n",
    "        \"num_classes\": 2,\n",
    "        \"num_pool\": 5,\n",
    "        \"num_conv_per_stage\": 2,\n",
    "        \"feat_map_mul_on_downscale\": 2,\n",
    "        \"norm_op_kwargs\": {'eps': 1e-05, 'affine': True},\n",
    "        \"dropout_op_kwargs\": {'p': 0, 'inplace': True},\n",
    "        \"nonlin_kwargs\": {'negative_slope': 0.01, 'inplace': True},\n",
    "        \"deep_supervision\": True,\n",
    "        \"dropout_in_localization\": False,\n",
    "        \"pool_op_kernel_sizes\": [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]],\n",
    "        \"conv_kernel_sizes\": [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]],\n",
    "        \"upscale_logits\": False,\n",
    "        \"convolutional_pooling\": True,\n",
    "        \"convolutional_upsampling\": True,\n",
    "    },\n",
    "    \"SlimUNETR\":{\n",
    "        \"in_channels\": 2,\n",
    "        \"out_channels\": 2,\n",
    "        \"embed_dim\": 96,\n",
    "        \"embedding_dim\": 32,\n",
    "    },\n",
    "    \"SegFormer\":{\n",
    "        \"in_channels\": 2,\n",
    "        \"num_classes\": 2\n",
    "    },\n",
    "    \"UNETRpp\":{\n",
    "        \"in_channels\": 2, \n",
    "         \"out_channels\": 2,\n",
    "        \"patch_size\" : [128, 128, 64],\n",
    "         \"feature_size\": 16,\n",
    "         \"hidden_size\": 256,\n",
    "         \"num_heads\": 4,\n",
    "         \"depths\": [3, 3, 3, 3],\n",
    "         \"dims\": [32, 64, 128, 256],\n",
    "         \"do_ds\": True\n",
    "    },\n",
    "    \"VSmTrans\":{\n",
    "        \"in_channels\": 2,\n",
    "        \"out_channels\": 2,\n",
    "        \"feature_size\": 12,\n",
    "        \"split_size\": [1, 2, 3, 4],\n",
    "        \"window_size\": 6,\n",
    "        \"num_heads\": [3, 6, 12, 24],\n",
    "        \"img_size\": [128, 128, 64],\n",
    "        \"depths\": [2, 2, 2, 2],\n",
    "        \"patch_size\": (2, 2, 2),\n",
    "        \"do_ds\": True\n",
    "    },\n",
    "    \n",
    "    \"HDense\":{\n",
    "        \"in_channels\": 2, \n",
    "        \"n_cls\": 2,\n",
    "        \"image_size\": (128, 128, 64),\n",
    "        \"transformer_depth\": 24\n",
    "    },\n",
    "    \n",
    "    \"SuperLightNet\":{\n",
    "        \"init_channels\": 2,\n",
    "        \"class_nums\": 2,\n",
    "        \"depths_unidirectional\": 'small'\n",
    "    },\n",
    "    \n",
    "    \"HCMA-UNet\":{\n",
    "        \"in_channels\": 2,\n",
    "        \"n_classes\": 2,\n",
    "        \"patch_ini\": [128, 128, 64],\n",
    "        \"predict_mode\": True\n",
    "    },\n",
    "    \n",
    "    \"U-KAN\":{\n",
    "        \"num_classes\": 2, \n",
    "        \"input_channels\": 2, \n",
    "        \"img_size\": [128, 128, 64], \n",
    "        \"embed_dims\": [128, 160, 256],\n",
    "    },\n",
    "    \n",
    "    \"U-RWKV\":{\n",
    "        \"input_channel\": 2, \n",
    "        \"num_classes\": 2\n",
    "    },\n",
    "    \n",
    "    \"VeloxSeg\":{\n",
    "        \"input_size\": [128, 128, 64],\n",
    "        \"patch_size\": 4,\n",
    "        \"in_ch\": [1, 1],\n",
    "        \"n_classes\": 2,\n",
    "        \"base_ch\": 16,\n",
    "        \n",
    "        \"conv_depths\": [1, 1, 1, 1],\n",
    "        \"kernel_sizes\": [1, 3, 5],\n",
    "        \"min_dim_group\": [4, 8, 8, 16],\n",
    "        \"conv_expansion_factor\": [3, 3, 2, 2],\n",
    "        \n",
    "        \"attn_base_ch\": 16, \n",
    "        \"depths\": [1, 1, 1, 1],\n",
    "        \"min_big_window_sizes\": [[4, 4, 2], [8, 8, 4], [4, 4, 2], [4, 4, 2]],\n",
    "        \"min_small_window_sizes\": [[1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1]],\n",
    "        \"min_dim_head\": [4, 8, 8, 16],\n",
    "        \"ffn_expansion_ratio\": [3, 3, 2, 2],\n",
    "        \n",
    "        \"proj_drop\":0.1,\n",
    "        \"conv_drop\":0.1,\n",
    "        \"spatial_dim\":  3\n",
    "    },\n",
    "}\n",
    "with open('./config/models_config_hecktor2022.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BraTS2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = {\n",
    "    \"UNet\":{\n",
    "        \"in_channels\": 4,\n",
    "        \"out_channels\": 4,\n",
    "    },\n",
    "    \"VNet\":{\n",
    "        \"in_channels\": 4,\n",
    "        \"out_channels\": 4,\n",
    "    },\n",
    "    \"MedNeXt\":{\n",
    "        \"num_input_channels\": 4, \n",
    "        \"num_classes\": 4, \n",
    "    },\n",
    "    \"UNETR\":{\n",
    "        \"in_channels\": 4,\n",
    "        \"out_channels\": 4,\n",
    "        \"img_size\": (96, 96, 96)\n",
    "    },\n",
    "    \"SwinUNETR\":{\n",
    "        \"in_channels\": 4,\n",
    "        \"out_channels\": 4\n",
    "    },\n",
    "    \"NestedFormer\":{\n",
    "        \"model_num\": 4,\n",
    "        \"out_channels\": 4,\n",
    "        \"image_size\": (96, 96, 96)\n",
    "    },\n",
    "    \"A2FSeg\":{\n",
    "        \"modality_num\": 4,\n",
    "        \"base_num_features\": 16,\n",
    "        \"num_classes\": 4,\n",
    "        \"num_pool\": 5,\n",
    "        \"num_conv_per_stage\": 2,\n",
    "        \"feat_map_mul_on_downscale\": 2,\n",
    "        \"norm_op_kwargs\": {'eps': 1e-05, 'affine': True},\n",
    "        \"dropout_op_kwargs\": {'p': 0, 'inplace': True},\n",
    "        \"nonlin_kwargs\": {'negative_slope': 0.01, 'inplace': True},\n",
    "        \"deep_supervision\": True,\n",
    "        \"dropout_in_localization\": False,\n",
    "        \"pool_op_kernel_sizes\": [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]],\n",
    "        \"conv_kernel_sizes\": [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]],\n",
    "        \"upscale_logits\": False,\n",
    "        \"convolutional_pooling\": True,\n",
    "        \"convolutional_upsampling\": True,\n",
    "    },\n",
    "    \"SlimUNETR\":{\n",
    "        \"in_channels\": 4,\n",
    "        \"out_channels\": 4,\n",
    "        \"embed_dim\": 96,\n",
    "        \"embedding_dim\": 27\n",
    "    },\n",
    "    \"SegFormer\":{\n",
    "        \"in_channels\": 4,\n",
    "        \"num_classes\": 4\n",
    "    },\n",
    "    \"UNETRpp\":{\n",
    "        \"in_channels\": 4, \n",
    "        \"out_channels\": 4,\n",
    "        \"patch_size\" : [96, 96, 96],\n",
    "        \"feature_size\": 16,\n",
    "        \"hidden_size\": 256,\n",
    "        \"num_heads\": 4,\n",
    "        \"depths\": [3, 3, 3, 3],\n",
    "        \"dims\": [32, 64, 128, 256],\n",
    "        \"do_ds\": True\n",
    "    },\n",
    "    \"VSmTrans\":{\n",
    "        \"in_channels\": 4,\n",
    "        \"out_channels\": 4,\n",
    "        \"feature_size\": 24,\n",
    "        \"split_size\": [1, 2, 3, 4],\n",
    "        \"window_size\": 6,\n",
    "        \"num_heads\": [3, 6, 12, 24],\n",
    "        \"img_size\": [96, 96, 96],\n",
    "        \"depths\": [2, 2, 2, 2],\n",
    "        \"patch_size\": (2, 2, 2),\n",
    "        \"do_ds\": True\n",
    "    },\n",
    "    \"HDense\":{\n",
    "        \"in_channels\": 4, \n",
    "        \"n_cls\": 4,\n",
    "        \"image_size\": (96, 96, 96),\n",
    "        \"transformer_depth\": 24\n",
    "    },\n",
    "    \n",
    "    \"SuperLightNet\":{\n",
    "        \"init_channels\": 4,\n",
    "        \"class_nums\": 4,\n",
    "        \"depths_unidirectional\": 'small'\n",
    "    },\n",
    "    \n",
    "    \"HCMA-UNet\":{\n",
    "        \"in_channels\": 4,\n",
    "        \"n_classes\": 4,\n",
    "        \"patch_ini\": [96, 96, 96],\n",
    "        \"predict_mode\": True\n",
    "    },\n",
    "    \n",
    "    \"U-KAN\":{\n",
    "        \"num_classes\": 4, \n",
    "        \"input_channels\": 4, \n",
    "        \"img_size\": [96, 96, 96], \n",
    "        \"embed_dims\": [128, 160, 256],\n",
    "    },\n",
    "    \n",
    "    \"U-RWKV\":{\n",
    "        \"input_channel\": 4, \n",
    "        \"num_classes\": 4,\n",
    "    },\n",
    "    \n",
    "    \"VeloxSeg\":{\n",
    "        \"input_size\": [96, 96, 96],\n",
    "        \"patch_size\": 4,\n",
    "        \"in_ch\": [4],\n",
    "        \"n_classes\": 4,\n",
    "        \"base_ch\": 16,\n",
    "        \n",
    "        \"conv_depths\": [1, 1, 1, 1],\n",
    "        \"kernel_sizes\": [1, 3, 5],\n",
    "        \"min_dim_group\": [4, 8, 8, 16],\n",
    "        \"conv_expansion_factor\": [3, 3, 2, 2],\n",
    "        \n",
    "        \"attn_base_ch\": 16, \n",
    "        \"depths\": [1, 1, 1, 1],\n",
    "        \"min_big_window_sizes\": [[3, 3, 3], [6, 6, 6], [3, 3, 3], [3, 3, 3]],\n",
    "        \"min_small_window_sizes\": [[1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1]],\n",
    "        \"min_dim_head\": [4, 8, 8, 16],\n",
    "        \"ffn_expansion_ratio\": [3, 3, 2, 2],\n",
    "        \"num_heads\": [1, 2, 2, 4],\n",
    "        \n",
    "        \"proj_drop\":0.1,\n",
    "        \"conv_drop\":0.1,\n",
    "        \"spatial_dim\": 3\n",
    "    },\n",
    "}\n",
    "with open('./config/models_config_brats2021.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
